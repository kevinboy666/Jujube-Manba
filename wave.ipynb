{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba\n",
    "import ptwt, pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaLayer(nn.Module):\n",
    "    def __init__(self, dim, d_state = 16, d_conv = 4, expand = 2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mamba = Mamba(\n",
    "                d_model=dim, # Model dimension d_model\n",
    "                d_state=d_state,  # SSM state expansion factor\n",
    "                d_conv=d_conv,    # Local convolution width\n",
    "                expand=expand,    # Block expansion factor\n",
    "        )\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     if x.dtype == torch.float16:\n",
    "    #         x = x.type(torch.float32)\n",
    "    #     B, C = x.shape[:2]\n",
    "    #     assert C == self.dim\n",
    "    #     n_tokens = x.shape[2:].numel()\n",
    "    #     img_dims = x.shape[2:]\n",
    "    #     x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)\n",
    "    #     x_norm = self.norm(x_flat)\n",
    "    #     x_mamba = self.mamba(x_norm)\n",
    "    #     out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)\n",
    "\n",
    "    #     return out\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float16:\n",
    "            x = x.type(torch.float32)\n",
    "        B, C, H, W = x.shape\n",
    "        print(H,W)\n",
    "        assert C == self.dim\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "\n",
    "        #expand the x\n",
    "        level = pywt.dwtn_max_level((H, W), 'db2')\n",
    "        level = 2 if level > 2 else 1\n",
    "\n",
    "        expand_x = torch.zeros(B, C, H if H%2==0 else H+1, W if W%2==0 else W+1, device=torch.device('cuda'))\n",
    "        expand_x[..., :H, :W] = x\n",
    "\n",
    "        coeff = ptwt.wavedec2(expand_x, 'db2', level=level)\n",
    "        \n",
    "        #set the initial list and append 1st element\n",
    "        flat_x = coeff[0]\n",
    "        dim_coef = [flat_x.shape[2:]]\n",
    "        token_coef = [flat_x.shape[2:].numel()]\n",
    "        flat_x = flat_x.reshape(B, C, -1)\n",
    "\n",
    "        for i in range(1, len(coeff)):\n",
    "            for j in range(3):\n",
    "                dim_coef.append(coeff[i][j].shape[2:])\n",
    "                token_coef.append(coeff[i][j].shape[2:].numel())\n",
    "                sub_coeff = coeff[i][j].reshape(B, C, -1)\n",
    "                flat_x = torch.cat((flat_x, sub_coeff), axis=2)\n",
    "\n",
    "        #list dict element, flatten and concatenate all elements\n",
    "        input = flat_x.transpose(-1, -2)\n",
    "        \n",
    "        #do normalization and mamba layer\n",
    "        x_norm = self.norm(input)\n",
    "        x_mamba = self.mamba(x_norm)\n",
    "        x_mamba = input + x_mamba\n",
    "\n",
    "        out_x = x_mamba.transpose(-1, -2)\n",
    "        out_x_split = list(torch.split(out_x, token_coef, dim=2))\n",
    "        \n",
    "        rec_x = [out_x_split.pop(0).reshape(B, C, dim_coef[0][0], dim_coef[0][1])]\n",
    "\n",
    "        for i in range(1, len(coeff)):\n",
    "            sub_rec_x = []\n",
    "            for j in range(3):\n",
    "                sub_rec_x.append(out_x_split.pop(0).reshape(B, C, dim_coef[i * 3 - 2 + j][0], dim_coef[i * 3 - 2 + j][1]))\n",
    "            rec_x.append(tuple(sub_rec_x))\n",
    "\n",
    "        rec_coeff = ptwt.waverec2(rec_x, 'db2')\n",
    "        raw_x = rec_coeff[..., :H, :W]\n",
    "\n",
    "        #reshape the output back to multiple dim coeff, inverse the wavelet and reshape to H*W*D\n",
    "        out = raw_x.reshape(B, C, *img_dims)\n",
    "        out = x + out\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
